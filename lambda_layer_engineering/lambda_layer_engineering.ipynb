{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c3d9db-975a-44fb-a784-d4b0e031715f",
   "metadata": {},
   "source": [
    "## Feature Engineering using Keras Lambda Layers for complete training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a00edd-fa52-4393-a2d4-f48dac6366e2",
   "metadata": {},
   "source": [
    "Often for structured data problems we end up using multiple libraries for preprocessing or feature engineering. We can go as far as having a full ML training pipeline using different libraries for example Pandas for reading data and also feature engineeering, sklearn for encoding features for example OneHot encoding and Normalization. The estimator might be an sklearn classifier, xgboost or it can for example be a Keras model. In the latter case, we would end up with artifacts for feature engineering and encoding and also different artifacts for the saved model. The pipeline is also disconnected and an extra step is needed to feed encoded data to the Keras model. For this step the data can be mapped from a dataframe to something like tf.data.Datasets type or numpy array before feeding it to a Keras model.\n",
    "\n",
    "In this post we will consider implementing a training pipeline natively with Keras/Tensorflow. From loading data with tf.data. As the the title suggested we will use Lambda layers for feature engineering. These engineered features will be stateless. For stateful preprocessing we could use something like Keras preprocessing layers. We will end up with a training pipeline where feature engineering will be part of the network architecture and can be persisted and loaded for inference as standalone.\n",
    "\n",
    "Keep in mind that tf.keras.layers.Lambda layers have (de)serialization limitations because Lambda layers are saved by serializing the python bytecode.\n",
    "\n",
    "Steps we will follow:\n",
    "- Load data with tf.data\n",
    "- Create Input layer\n",
    "- Create feature layer using Lambda layers\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e78ef-e9d9-4f0c-9bcc-d027f91f1de7",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d1559-a7ab-4c3e-94f9-c797e2b20a5c",
   "metadata": {},
   "source": [
    "For the example below we will use the heart disease dataset. Lets import tensorflow and read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c761ac5d-0928-4427-8ea6-489a55ef3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-22 18:33:22.765978: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "  \n",
    "binary_features = ['sex', 'fbs', 'exang']\n",
    "numeric_features =  ['trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'cp', 'restecg', 'ca']\n",
    "categoric_features = ['thal']\n",
    "\n",
    "dtype_mapper = {\n",
    "        'age': tf.float32,\n",
    "        'sex': tf.float32,\n",
    "        'cp': tf.float32,\n",
    "        'trestbps': tf.float32,\n",
    "        'chol': tf.float32,\n",
    "        'fbs': tf.float32,\n",
    "        'restecg': tf.float32,\n",
    "        'thalach': tf.float32,\n",
    "        'exang': tf.float32,\n",
    "        'oldpeak': tf.float32,\n",
    "        'slope': tf.float32,\n",
    "        'ca': tf.float32,\n",
    "        'thal': tf.string\n",
    "}\n",
    "\n",
    "heart_dir = tf.keras.utils.get_file(\"heart.csv\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\")\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "      heart_dir,\n",
    "      batch_size=64,\n",
    "      label_name='target',\n",
    "      num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2bf6a-d8bc-41cf-a447-ea5261596adf",
   "metadata": {},
   "source": [
    "#### Create a dictionary of Input objects for each feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a39776-16a3-410f-bed4-c04b0ec40198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs(data_type_mapper):\n",
    "    \"\"\"Create model inputs\n",
    "    Args:\n",
    "        data_type_mapper (dict): Dictionary with feature as key and dtype as value\n",
    "                                 For example {'age': tf.float32, ...}\n",
    "    Returns:\n",
    "        (dict): Keras inputs for each feature\n",
    "    \"\"\"\n",
    "    return {feature: tf.keras.Input(shape=(1,), name=feature, dtype=dtype)\\\n",
    "        for feature, dtype in data_type_mapper.items()}\n",
    "\n",
    "feature_layer_inputs = create_inputs(dtype_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23354176-aa5b-4ac0-bcc4-6c5887487460",
   "metadata": {},
   "source": [
    "#### We will be using Lambda layers in this example for feature engineering. Below are the functions that we will use to create our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49391675-d476-427b-9b01-2ddc49a9ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for engineered features\n",
    "\n",
    "def square(x):\n",
    "  \"\"\"apply sqaure of feature\"\"\"\n",
    "  return x ** 2\n",
    "\n",
    "def ratio(x):\n",
    "  \"\"\"compute the ratio between two numeric features\"\"\"\n",
    "  return x[0] / x[1]\n",
    "\n",
    "def cross_feature(x):\n",
    "  \"\"\"compute the crossing of two features\"\"\"\n",
    "  return tf.cast(x[0] * x[1], dtype = tf.float32)\n",
    "\n",
    "def age_and_gender(x):\n",
    "  \"\"\"check if age gt 50 and if gender is male\"\"\"\n",
    "  return tf.cast(\n",
    "    tf.math.logical_and(x[0] > 50, x[1] == 1), dtype = tf.float32\n",
    "  )\n",
    "\n",
    "def is_fixed(x):\n",
    "  \"\"\"encode categoric feature if value is equal to fixed\"\"\"\n",
    "  return tf.cast(x == 'fixed', dtype = tf.float32)\n",
    "\n",
    "def is_reversible(x):\n",
    "  \"\"\"encode categoric feature if value is equal to fixed\"\"\"\n",
    "  return tf.cast(x == 'reversible', dtype = tf.float32)\n",
    "\n",
    "def is_normal(x):\n",
    "  \"\"\"encode categoric feature if value is equal to fixed\"\"\"\n",
    "  return tf.cast(x == 'normal', dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1db2e8-ceef-4b9c-aa51-5f0312503a8e",
   "metadata": {},
   "source": [
    "### Now that we have our functions lets create the features as Lambda layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c0767d-e57b-45ad-b407-e5e108e85685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features based on thal is similar to one-hot encoding. \n",
    "# Here we only illustrate using lambda layers\n",
    "\n",
    "is_fixed = tf.keras.layers.Lambda(is_fixed)(\n",
    "   feature_layer_inputs['thal']\n",
    ")\n",
    "\n",
    "is_normal = tf.keras.layers.Lambda(is_normal)(\n",
    "   feature_layer_inputs['thal']\n",
    ")\n",
    "\n",
    "is_reversible = tf.keras.layers.Lambda(is_reversible)(\n",
    "   feature_layer_inputs['thal']\n",
    ")\n",
    "\n",
    "age_and_gender = tf.keras.layers.Lambda(age_and_gender)(\n",
    "    (feature_layer_inputs['age'], feature_layer_inputs['sex'])\n",
    ")\n",
    "\n",
    "age = tf.keras.layers.Lambda(lambda x: tf.cast(x > 50, dtype = tf.float32))(\n",
    "    feature_layer_inputs['age']\n",
    ")\n",
    "\n",
    "trest_chol_ratio = tf.keras.layers.Lambda(ratio, name='trest_chol_ratio')(\n",
    "   (feature_layer_inputs['trestbps'], feature_layer_inputs['chol'])\n",
    ")\n",
    "\n",
    "trest_cross_thalach = tf.keras.layers.Lambda(cross_feature)(\n",
    "   (feature_layer_inputs['trestbps'], feature_layer_inputs['thalach'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b1c55-9a94-4ac4-83b6-b01d799e6dd3",
   "metadata": {},
   "source": [
    "#### All our engineered feature layers are created and we can now combine it with our other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5f1cee-9b81-48c7-8b9b-190e4c202e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all newly created features into one layer\n",
    "lambda_feature_layer = tf.keras.layers.concatenate(\n",
    "    [is_fixed, is_normal, is_reversible, \n",
    "     age, age_and_gender, trest_chol_ratio, trest_cross_thalach]\n",
    ")\n",
    "\n",
    "numeric_feature_layer = tf.keras.layers.concatenate(\n",
    "    [feature_layer_inputs[feature] for feature in numeric_features]\n",
    ")\n",
    "\n",
    "binary_feature_layer = tf.keras.layers.concatenate(\n",
    "    [feature_layer_inputs[feature] for feature in binary_features]\n",
    ")\n",
    "\n",
    "# Add the rest of features\n",
    "feature_layer = tf.keras.layers.concatenate(\n",
    "    [lambda_feature_layer, numeric_feature_layer, binary_feature_layer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cfe01-fbed-4601-8ae7-fb88adacd80f",
   "metadata": {},
   "source": [
    "#### Our last step is to create and fit our Keras model. For this example we will use a simple model architecture. We will persist the model and load it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ff9018-0774-4713-80a4-5b2ec10efa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 11ms/step - loss: 0.4536 - accuracy: 0.7858 - auc: 0.8430\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8558 - auc: 0.9210\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8650 - auc: 0.9308\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8660 - auc: 0.9321\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8647 - auc: 0.9308\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8650 - auc: 0.9290\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3013 - accuracy: 0.8693 - auc: 0.9319\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8660 - auc: 0.9318\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.8700 - auc: 0.9324\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8680 - auc: 0.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-22 18:36:16.318756: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lambda_layered_model/assets\n",
      "48/48 [==============================] - 1s 4ms/step - loss: 0.2969 - accuracy: 0.8680 - auc: 0.9341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2968518137931824, 0.867986798286438, 0.9341182708740234]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup model, this is basically Logistic regression\n",
    "x = tf.keras.layers.BatchNormalization()(feature_layer)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=feature_layer_inputs, outputs=output)\n",
    "model.compile(\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), \n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "model.fit(dataset, epochs=10)\n",
    "\n",
    "# save model\n",
    "tf.keras.models.save_model(model, \"lambda_layered_model\")\n",
    "\n",
    "# load model for inference\n",
    "loaded_model = tf.keras.models.load_model(\"lambda_layered_model\")\n",
    "loaded_model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf2745-b622-48d8-88cf-5e062cd7222f",
   "metadata": {},
   "source": [
    "#### To conclude we were able to successfully build a model using Keras Lambda layers. This model was saved and loaded for inference. Our feature engineering is part of our saved model(model architecture). Everything natively in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2351af5-6a17-4e29-b950-e654d8d4f7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
