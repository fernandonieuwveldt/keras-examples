{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering with a custom Keras Feature Layer for complete Training and Inference Pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example is similar to the lambda_layer_engineering.ipynb notebook. But here we will implement a single custom layer by subclassing Keras Layer class.\n",
    "\n",
    "The idea is similar to the reference notebook above. But lets set the scene again:\n",
    "\n",
    "Often for structured data problems we end up using multiple libraries for preprocessing or feature engineering. We can go as far as having a full ML training pipeline using different libraries for example Pandas for reading data and also feature engineeering, sklearn for encoding features for example OneHot encoding and Normalization. The estimator might be an sklearn classifier, xgboost or it can for example be a Keras model. In the latter case, we would end up with artifacts for feature engineering and encoding and also different artifacts for the saved model. The pipeline is also disconnected and an extra step is needed to feed encoded data to the Keras model. For this step the data can be mapped from a dataframe to something like tf.data.Datasets type or numpy array before feeding it to a Keras model.\n",
    "\n",
    "In this post we will consider implementing a training pipeline natively with Keras/Tensorflow. From loading data with tf.data and applying feature engineering in a single custom layer by subclassing Layer class. These engineered features will be stateless. For stateful preprocessing we could use something like Keras preprocessing layers. We will end up with a training pipeline where feature engineering will be part of the network architecture and can be persisted and loaded for inference as standalone.\n",
    "\n",
    "Steps we will follow:\n",
    "- Load data with tf.data\n",
    "- Create Input layer\n",
    "- Implementing a custom Feature Layer\n",
    "- Train model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the example below we will use the heart disease dataset. Lets import tensorflow and read in the data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "heart_dir = tf.keras.utils.get_file(\n",
    "    \"heart.csv\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    ")\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "      heart_dir,\n",
    "      batch_size=64,\n",
    "      label_name='target',\n",
    "      num_epochs=10\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "binary_features = ['sex', 'fbs', 'exang']\n",
    "numeric_features =  ['trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'cp', 'restecg', 'ca']\n",
    "categoric_features = ['thal']\n",
    "\n",
    "dtype_mapper = {\n",
    "        'age': tf.float32,\n",
    "        'sex': tf.float32,\n",
    "        'cp': tf.float32,\n",
    "        'trestbps': tf.float32,\n",
    "        'chol': tf.float32,\n",
    "        'fbs': tf.float32,\n",
    "        'restecg': tf.float32,\n",
    "        'thalach': tf.float32,\n",
    "        'exang': tf.float32,\n",
    "        'oldpeak': tf.float32,\n",
    "        'slope': tf.float32,\n",
    "        'ca': tf.float32,\n",
    "        'thal': tf.string\n",
    "}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Input Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def create_inputs(data_type_mapper):\n",
    "    \"\"\"Create model inputs\n",
    "    Args:\n",
    "        data_type_mapper (dict): Dictionary with feature as key and dtype as value\n",
    "                                 For example {'age': tf.float32, ...}\n",
    "    Returns:\n",
    "        (dict): Keras inputs for each feature\n",
    "    \"\"\"\n",
    "    return {feature: tf.keras.Input(shape=(1,), name=feature, dtype=dtype)\\\n",
    "        for feature, dtype in data_type_mapper.items()}\n",
    "\n",
    "feature_layer_inputs = create_inputs(dtype_mapper)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create custom Feature Layer for Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class FeatureLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom Layer for Feature engineering steps\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(FeatureLayer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        age_and_gender = tf.cast(\n",
    "            tf.math.logical_and(inputs['age'] > 50, inputs['sex'] == 1), dtype = tf.float32\n",
    "        )\n",
    "\n",
    "        thal_fixed_category = tf.cast(inputs['thal'] == \"fixed\", dtype = tf.float32)\n",
    "        thal_reversible_category = tf.cast(inputs['thal'] == \"reversible\", dtype = tf.float32)\n",
    "        thal_normal_category = tf.cast(inputs['thal'] == \"normal\", dtype = tf.float32)\n",
    "\n",
    "        trest_chol_ratio = inputs['trestbps'] / inputs['chol']\n",
    "        trest_cross_thalach = inputs['trestbps'] * inputs['thalach']\n",
    "\n",
    "        # concat all newly created features into one layer\n",
    "        feature_list = [thal_fixed_category, thal_reversible_category, thal_normal_category,\n",
    "                        age_and_gender, trest_chol_ratio, trest_cross_thalach]\n",
    "\n",
    "        engineered_feature_layer = tf.keras.layers.concatenate(feature_list, name='engineered_feature_layer')\n",
    "        numeric_feature_layer = tf.keras.layers.concatenate(\n",
    "            [inputs[feature] for feature in numeric_features], name='numeric_feature_layer'\n",
    "        )\n",
    "\n",
    "        binary_feature_layer = tf.keras.layers.concatenate(\n",
    "            [inputs[feature] for feature in binary_features], name='binary_feature_layer'\n",
    "        )\n",
    "\n",
    "        # Add the rest of features into final feature layer\n",
    "        feature_layer = tf.keras.layers.concatenate(\n",
    "            [engineered_feature_layer, numeric_feature_layer, binary_feature_layer], name='feature_layer'\n",
    "        )\n",
    "        return feature_layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model and Save model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our last step is to create and fit our Keras model. For this example we will use a simple model architecture. We will persist the model and load it for inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# setup model, this is basically Logistic regression\n",
    "feature_layer = FeatureLayer()(feature_layer_inputs)\n",
    "x = tf.keras.layers.BatchNormalization(name='batch_norm')(feature_layer)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='target')(x)\n",
    "model = tf.keras.Model(inputs=feature_layer_inputs, outputs=output)\n",
    "model.compile(\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "  metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), \n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model.fit(dataset, epochs=10)\n",
    "\n",
    "# save model\n",
    "tf.keras.models.save_model(model, \"saved_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "48/48 [==============================] - 1s 5ms/step - loss: 0.7724 - accuracy: 0.6323 - auc: 0.5312\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 600us/step - loss: 0.3704 - accuracy: 0.8290 - auc: 0.8900\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 562us/step - loss: 0.3359 - accuracy: 0.8495 - auc: 0.9121\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 570us/step - loss: 0.3317 - accuracy: 0.8554 - auc: 0.9152\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 568us/step - loss: 0.3239 - accuracy: 0.8574 - auc: 0.9200\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 583us/step - loss: 0.3198 - accuracy: 0.8574 - auc: 0.9217\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 598us/step - loss: 0.3121 - accuracy: 0.8630 - auc: 0.9253\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 600us/step - loss: 0.3070 - accuracy: 0.8637 - auc: 0.9294\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 614us/step - loss: 0.3058 - accuracy: 0.8657 - auc: 0.9295\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 834us/step - loss: 0.3056 - accuracy: 0.8640 - auc: 0.9291\n",
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model and predict on raw data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# load model for inference\n",
    "loaded_model = tf.keras.models.load_model(\"saved_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "dict(zip(loaded_model.metrics_names,loaded_model.evaluate(dataset)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48/48 [==============================] - 0s 742us/step - loss: 0.2992 - accuracy: 0.8713 - auc: 0.9336\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 0.29918530583381653,\n",
       " 'accuracy': 0.8712871074676514,\n",
       " 'auc': 0.9335706830024719}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6174d289b1d1622648b9ec05c2563af9ce43abcb1609ba8e2ef50bca8f1f1af"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit ('data-science': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}